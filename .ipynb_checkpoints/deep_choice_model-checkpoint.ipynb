{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "paramters \n",
    "------\n",
    "\n",
    "n_input: int\n",
    "    number of units of input layer, corresponses the ingreident choice set\n",
    "\n",
    "n_output: int\n",
    "    number of units of output layer, the essential part of a training sampler, {N: the input ingredients, M: the output ingredient}\n",
    "\n",
    "n_hidden: int\n",
    "    number of hidden units\n",
    "    \n",
    "kk: int\n",
    "    number of steps within contrastive divergence\n",
    "    \n",
    "momentum: bool\n",
    "    whether to use momentum\n",
    "    \n",
    "GB: bool\n",
    "    if true, use a Gaussian-Bernoulli (real-valued input and ouput units) model, otherwise use a Bernoulli-Bernoulli (binary) model  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_input, n_output, n_hidden, kk = 1, momentum = False, GB = False):\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_visible = n_input + n_output\n",
    "        self.n_hidden = n_hidden\n",
    "        self.kk = kk\n",
    "        \n",
    "        # weights and biases\n",
    "        self.ww = tf.Variable(tf.uniform([self.n_visible, n_hidden], minval = 0.0, maxval = 1.0, dtype = tf.float32), name = \"visible_hidden_weights\")\n",
    "        self.vb = tf.Variable(tf.zeros([self.n_visible]) + 0.1, dtype = tf.float32, name = \"visible_biases\")\n",
    "        self.hb = tf.Variable(tf.zeros([n_hidden]) + 0.1, dtype = tf.float32, name = \"hidden_biases\")\n",
    "        \n",
    "        # update velocities\n",
    "        self.ww_v = tf.Variable(tf.zeros([self.n_visible, n_hidden]), dtype = tf.float32, name = \"weight_speed\") \n",
    "        self.vb_v = tf.Variable(tf.zeros([self.n_visible]), dtype = tf.float32, name = \"visible_bias_speed\") \n",
    "        self.hb_v = tf.Variable(tf.zeros([n_hidden]), dtype = tf.float32, name = \"hidden_bias_speed\")  \n",
    "\n",
    "        # Momentum method\n",
    "        if momentum:\n",
    "            self.momentum = tf.placeholder(tf.float32)\n",
    "        else:\n",
    "            self.momentum = 0.0\n",
    "    \n",
    "        self.vv = tf.placeholder(tf.int8)\n",
    "        \n",
    "        \n",
    "        '''naive tran_step'''\n",
    "\n",
    "        def train_step(self, train_data):\n",
    "            dw, dvb, dhb = self.CD(train_data)\n",
    "            \n",
    "        \n",
    "        '''    \n",
    "        # train with momentum\n",
    "        def train_step(self, train_data):\n",
    "        dw, dvb, dhb = self.CD(train_data)\n",
    "        \n",
    "        # momentum method: v = - gradient * lr + v * momemtum\n",
    "        new_ww_v = w_grad * self.lr + self.ww_v * momentum\n",
    "        new_vb_v = vb_grad * self.lr + self.vb_v * momentum\n",
    "        new_hb_v = hb_grad * self.lr + self.vb_v * momentum\n",
    "        # update velocity        \n",
    "        update_ww_v = tf.assign(self.ww_v, new_ww_v)\n",
    "        update_vb_v = tf.assign(self.vb_v, new_vb_v)\n",
    "        update_hb_v = tf.assign(self.hb_v, new_hb_v)\n",
    "        # update weights and biases\n",
    "        update_ww = tf.assign(self.ww, self.ww + new_ww_v)\n",
    "        update_vb = tf.assign(self.vb, self.vb + new_vb_v)\n",
    "        update_vb = tf.assign(sef.hb, self.hb + new_hb_v)\n",
    "        '''             \n",
    "\n",
    "\n",
    "    def CD(self, vv):\n",
    "        '''Constrastive divergence with k steps using Gibbs sampling'''\n",
    "        hh_prob = hh_prob_given_v(vv)\n",
    "        hh = sample_hh(hh_prob)\n",
    "\n",
    "        \n",
    "        new_vv_prob = vv_prob_given_hh(hh)\n",
    "        new_vv = sample_vv(new_vv_prob)\n",
    "        new_hh_prob = hh_prob_given_vv(new_vv)\n",
    "        new_hh = sample_hh(new_hh_prob)\n",
    "        \n",
    "        for ii in range(self.kk):\n",
    "            new_vv_prob = vv_prob_given_hh(hh)\n",
    "            new_vv = sample_v(new_vv_prob)\n",
    "            new_hh_prob = hh_prob_given_vv(new_vv)\n",
    "            new_hh = sample_hh(new_hh_prob)\n",
    "        \n",
    "        \n",
    "        # positive divergence  h(v).v^T\n",
    "        pos_div = tf.matmul(tf.tranpose(vv), hh_prob)\n",
    "        # negative divergence h(v').v'^T\n",
    "        neg_div = tf.matmul(tf.tranpose(new_vv), new_hh_prob)\n",
    "        # approaximate the gradients    \n",
    "        dw = pos_div - neg_div\n",
    "        dvb = vv - new_vv\n",
    "        dhb = hh_prob - new_hh_prob\n",
    "        return dw, dvb, dhb\n",
    "    \n",
    "    \n",
    "    # return sigm(weight dot visibles + hidden_biases)\n",
    "    def hh_prob_given_vv(self, vv):\n",
    "        return tf.nn.sigmoid(tf.matmul(vv, self.vh_weight) + self.h_bias)\n",
    "        \n",
    "    def vv_prob_given_hh(self, hh):\n",
    "        return tf.nn.sigmoid(tf.matmul(hh, tf.transpose(self.vh_weight)) + self.v_bias)\n",
    "    \n",
    "    def sample_hh(self, vv_prob):\n",
    "        hh = tf.round(vv_prob)\n",
    "        return hh\n",
    "    \n",
    "    def sample_vv(self, hh_prob):\n",
    "        vv = tf.round(hh_prob)\n",
    "        assert (vv == 1 or vv == 0) \n",
    "        return vv\n",
    "        \n",
    "    def free_energy():\n",
    "        pass\n",
    "    \n",
    "    def pseudo_log_likelihood(self, visible):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "paramters \n",
    "------\n",
    "\n",
    "n_choice: int\n",
    "    number of units of choice set layer, corresponses the ingredient choice set\n",
    "\n",
    "n_selected: int\n",
    "    number of units of output layer, the essential part of a training sampler, {N: the input ingredients, M: the output ingredient}\n",
    "\n",
    "n_hidden: int\n",
    "    number of hidden units\n",
    "    \n",
    "n_flavor: int\n",
    "    number of flavor compounds, also number of features\n",
    "    \n",
    "n_batch: int\n",
    "    mini-batch size for each PCD\n",
    "    \n",
    "kk: int\n",
    "    usually same as n_batch\n",
    "    \n",
    "choice_set: {n_ingredient, n_flavor}, contains ingredients and their flavor compound information\n",
    "\n",
    "recipes: {n_sample, n_ingredient, n_flavor}, n recipes, each contains their ingredients \n",
    "    \n",
    "GB: bool\n",
    "    if true, use a Gaussian-Bernoulli (real-valued input and ouput units) model, otherwise use a Bernoulli-Bernoulli (binary) model  \n",
    "\"\"\"\n",
    "\n",
    "class B_DRBM:\n",
    "    def __init__(self, choice_set, recipes, flavors, n_choice, n_selected, n_hidden, n_flavor, lr  kk = 1, n_batch = 1, momentum = True):\n",
    "        self.n_choice = n_choice\n",
    "        self.n_selected = n_selected\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_flavor = n_flavor\n",
    "        self.n_batch = n_batch\n",
    "        self.kk = kk\n",
    "       \n",
    "        # weights and biases\n",
    "        self.w_in = tf.Variable(tf.random_normal([n_choice, n_hidden], mean=0, stddev=1), name = \"input_hidden_weight\")\n",
    "        self.w_out = tf.Variable(tf.truncated_normal([n_selected, n_hidden], mean=0, stddev=1), nmae = \"output_hidden_weight\")\n",
    "        self.b_h = tf.Variable(tf.zeros([self.n_hidden]) + 0.1, dtype = tf.float32, name = \"hidden_bias\")\n",
    "        self.b_in = tf.constant(tf.zeros[self.n_choice], dtype = tf.float32, name = \"None_input_bias\")\n",
    "        self.b_out = tf.Variable(tf.zeros([self.n_selected]) + 0.1, dtype = tf.float32, name = \"output_bias\")\n",
    "\n",
    "        \n",
    "        # momentum method: v = - gradient * lr + v * momemtum\n",
    "        #new_ww_v = w_grad * self.lr + self.ww_v * momentum\n",
    "        #new_vb_v = vb_grad * self.lr + self.vb_v * momentum\n",
    "        #new_hb_v = hb_grad * self.lr + self.vb_v * momentum\n",
    "        # update velocity        \n",
    "        #update_ww_v = tf.assign(self.ww_v, new_ww_v)\n",
    "        #update_vb_v = tf.assign(self.vb_v, new_vb_v)\n",
    "        #update_hb_v = tf.assign(self.hb_v, new_hb_v)\n",
    "        \n",
    "        '''\n",
    "        self.ww = tf.Variable(tf.uniform([self.n_visible, n_hidden], minval = 0.0, maxval = 1.0, dtype = tf.float32), name = \"visible_hidden_weights\")\n",
    "        self.vb = tf.Variable(tf.zeros([self.n_visible]) + 0.1, dtype = tf.float32, name = \"visible_bias\")\n",
    "        self.hb = tf.Variable(tf.zeros([n_hidden]) + 0.1, dtype = tf.float32, name = \"hidden_bias\")\n",
    "        '''\n",
    "\n",
    "    # compute average feature of choice set or a single recipe\n",
    "    def get_mean_choice(self, choice_set):\n",
    "        return tf.reduce_mean(choice_set, 0)\n",
    "        \n",
    "        \n",
    "    def get_mean_recipes(self, recipes):\n",
    "        return tf.reduce_mean(choice_set, 1)\n",
    "    \n",
    "    \n",
    "    def train(self, choice_set, recipes):\n",
    "        '''        \n",
    "        parameters\n",
    "        ---------------------\n",
    "        zz: [n_ingr, n_flavor]\n",
    "            all ingredients and flavor set, choice set, shap\n",
    "        xxs:[n_sample, n_ingr, n_flaovr]\n",
    "            mutilple recipes for training step\n",
    "        '''\n",
    "        # update for each batch\n",
    "        nn = tf.shape(recipes)(0) // self.n_batch\n",
    "        zz = get_mean_choice(choice_set)\n",
    "        \n",
    "        for ii in range(nn):\n",
    "            ### make sure it is the right slice\n",
    "            batch = tf.slice(recipes, [nn * self.n_batch, self.n_choice, self.n_flavor], [(nn + 1) * self.n_batch, self.n_choice, self.n_flavor])\n",
    "            xxs = get_mean_recipes(batch)\n",
    "            last_v_sample = zz + xxs[0, :]\n",
    "            \n",
    "            for jj in self.n_batch:\n",
    "                xx = xxs[jj, :]\n",
    "                v_sample = zz + xx\n",
    "                dw_in, dw_out, db_h, db_out, last_v_sample = self.PCD(v_sample, last_v_sample)\n",
    "                update(dw_in, dw_out, db_h, db_out)\n",
    "                # compute pseudo_log_likelihood for tracking the performance\n",
    "\n",
    "        \n",
    "        \n",
    "    def PCD(self, v_sample, last_v_sample):\n",
    "        # Persietent constrastive divergence with k steps using Gibbs sampling for data samples in a mini-batch\n",
    "        concate_weight = tf.concat([self.w_in, self.w_out], 0)\n",
    "        concate_v_bias = tf.concat([self.b_in, self.b_out], 0)\n",
    "        \n",
    "        # Markov chain starts from the visibles of the latest gibbs sample\n",
    "        h_prob = h_prob_give_v(last_v_sample, concate_weight, h_bias)\n",
    "        h_sample = sample_h(h_prob)            \n",
    "        new_v_prob = v_prob_give_h(h_sample, concate_weight, concate_v_bias)\n",
    "        new_v_sample = sample_v(new_v_prob)\n",
    "\n",
    "        for ii in range(self.n_batch - 1):\n",
    "            new_h_prob = h_prob_give_v(new_v_sample)\n",
    "            new_h_sample = sample(new_h_prob)\n",
    "            new_v_prob = v_prob_give_h(new_h_sample)\n",
    "            new_v_sample = sample_v(new_v_prob)\n",
    "\n",
    "        last_v_sample = new_v_sample\n",
    "\n",
    "        # Update for each data sample inside a batch\n",
    "        # positive divergence  h(v).v^T     negative divergence: db_h = h_prob - new_h_prob\n",
    "        pos_div = tf.matmul(h_prob, tf.tranpose(v_sample))\n",
    "        neg_div = tf.matmul(new_h_prob, tf.tranpose(new_v_sample) )\n",
    "\n",
    "        dw = pos_div - neg_div\n",
    "        db_h = h_prob - new_h_prob\n",
    "        db_v = v_sample - new_v_sample\n",
    "        \n",
    "        dw_in, dw_out = tf.split(dw, [self.n_choice, self.n_selected], 0) # split input and output weights\n",
    "        db_in, db_out = tf.split(dv, [self.n_choice, self.n_selected], 0) # split bias\n",
    "        \n",
    "        return dw_in, dw_out, db_h, db_out, last_v_sample\n",
    "    \n",
    "    \n",
    "    # return sigm(weight dot visibles + hidden_biases)\n",
    "    def h_prob_given_v(v_sample, concate_weight, h_bias):\n",
    "        # merge two weights\n",
    "        return tf.nn.sigmoid(tf.matmul(v_sample, concate_weight) + h_bias)\n",
    "        \n",
    "    def v_prob_given_h(h_sample, concate_weight, concate_v_bias):\n",
    "        return tf.nn.sigmoid(tf.matmul(h_sample, tf.transpose(concate_weight)) + concate_v_bias)\n",
    "    \n",
    "    def sample_h(v_prob):\n",
    "        h_sample = tf.round(v_prob)\n",
    "        return h_sample\n",
    "    \n",
    "    def sample_v(h_prob):\n",
    "        vv_sample = tf.round(h_prob)\n",
    "        assert (v_sample == 1 or v_sample == 0) \n",
    "        return v_sample\n",
    "        \n",
    "    def update(dw_in, dw_out, db_h, db_out):\n",
    "        update_w_in = tf.assign(self.w_in, self.w_in + tf.matmul(dw_in, self.lr))\n",
    "        update_w_out = tf.assign(self.w_out, self.w_out + tf.matmul(dw_out, self.lr))\n",
    "        update_b_h = tf.assign(self.b_h, self.b_h + tf.matmul(db_h, self.lr))\n",
    "        update_b_out = tf.assign(self.b_out, self.b_out + tf.matmul(db_out, self.lr))\n",
    "        \n",
    "    def free_energy(self, v_sample):\n",
    "        ww = tf.concat([self.w_in, self.w_out], 0)\n",
    "        v_b = tf.concat([self.b_in, self.b_out], 0)\n",
    "        \n",
    "        vb_term = - tf.matmul(v_sample, tf.transpose(v_b))\n",
    "        hidden_term = tf.reduce_sum(tf.log(1 + tf.exp(tf.matmul(v_sample, ww) + self.h_b)))\n",
    "    \n",
    "    def pseudo_llh(self, v_sample):\n",
    "        # pseudo_log_likelihood\n",
    "        v_sample_flip_i = v_sample\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "data = np.array(pd.read_csv(r\".\\data\\test_recip_ingr.csv\", nrows = 50))\n",
    "ingr_flavor = np.array(pd.read_csv(r\".\\data\\matrix_ingr_comp.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_choice = 1\n",
    "n_flavor = 2\n",
    "\n",
    "self.choice_set = tf.placeholder(tf.float32, shape=[n_choice, n_flavor])\n",
    "self.recipes = tf.placeholder(tf.float32, shape=[None, n_choice, n_flavor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1530, 1107)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr_flavor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.constant([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.slice(s, [1], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a5fecba28a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_choice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_flavor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "batch = tf.slice(data, [0 * self.n_batch], [self.n_choice, self.n_flavor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([[1,2,3], [4,5,6]])\n",
    "b = tf.Variable([[7,8,9], [10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.concat([a,b], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1530)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
